import nest_asyncio
nest_asyncio.apply()

import streamlit as st
import fitz
import logging
import requests

from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.base import BaseCallbackHandler
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate

# –ò–º–ø–æ—Ä—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ LLM
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–µ—à (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤ LLM –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤ –ø–∞–º—è—Ç–∏)
set_llm_cache(InMemoryCache())

logging.basicConfig(level=logging.INFO)


# Telegram –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

TELEGRAM_BOT_TOKEN = "7747155220:AAGYM6f6OePOmFUrWX5u9fVvuIydY7qKB-c"
TELEGRAM_CHAT_ID = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à Telegram Chat ID:")

def send_telegram_message_sync(chat_id, message):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram —á–µ—Ä–µ–∑ HTTP-–∑–∞–ø—Ä–æ—Å."""
    max_length = 4096
    if len(message) > max_length:
        message = message[:max_length - 10] + "..."
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    data = {"chat_id": chat_id, "text": message}
    try:
        response = requests.post(url, data=data)
        if response.status_code != 200:
            logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {response.text}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")


# Callback –¥–ª—è Telegram —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Å–ª—É–∂–µ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

class TelegramCallbackHandler(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        answer_text = ""
        if isinstance(response, dict):
            if "answer" in response:
                answer_text = response["answer"]
            elif "generations" in response:
                gens = response["generations"]
                if isinstance(gens, list) and len(gens) > 0:
                    first_chunk_list = gens[0]
                    if isinstance(first_chunk_list, list) and len(first_chunk_list) > 0:
                        answer_text = first_chunk_list[0].text
                    else:
                        answer_text = str(gens)
                else:
                    answer_text = str(response)
            else:
                answer_text = str(response)
        else:
            answer_text = str(response)
        
        if TELEGRAM_CHAT_ID:
            message = f"üì¢ LLM –∑–∞–≤–µ—Ä—à–∏–ª –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞!\n\n–û—Ç–≤–µ—Ç: {answer_text}"
            send_telegram_message_sync(TELEGRAM_CHAT_ID, message)


# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ LangChain

def filter_swear_words_langchain(text: str) -> str:
    template = (
        "–ü—Ä–æ–≤–µ—Ä—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö —Å–ª–æ–≤.\n"
        "–°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö —Å–ª–æ–≤: –¥—É—Ä–∞–∫, –∏–¥–∏–æ—Ç, —Ç—É–ø–æ–π, —É—Ä–æ–¥, –∫–æ–∑–µ–ª, —Å–≤–æ–ª–æ—á—å, idiot, stupid, shit, bastard\n"
        "–ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–¥–Ω–æ –∏–ª–∏ –±–æ–ª–µ–µ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö —Å–ª–æ–≤, –≤—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ 'BAD_WORD_FOUND'.\n"
        "–ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö —Å–ª–æ–≤, –≤—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ 'OK'.\n"
        "–¢–µ–∫—Å—Ç: {text}"
    )
    prompt = PromptTemplate(input_variables=["text"], template=template)
    chain = prompt | OllamaLLM(model="llama3.2:latest")
    response = chain.invoke({"text": text}).strip()
    if "BAD_WORD_FOUND" in response:
        if TELEGRAM_CHAT_ID:
            send_telegram_message_sync(TELEGRAM_CHAT_ID, "‚ö†Ô∏è –ê–π-–∞–π-–∞–π, —Ç–∞–∫ –Ω–µ–ª—å–∑—è –ø–∏—Å–∞—Ç—å –ø–ª–æ—Ö–∏–µ —Å–ª–æ–≤–∞!")
        return None
    return text


# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF

def extract_text_from_pdf(uploaded_file):
    try:
        uploaded_file.seek(0)
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        text = "\n".join([page.get_text("text") for page in pdf_document])
        return text.strip()
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ PDF: {e}")
        return None


# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞

def create_vector_store(text):
    splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200)
    texts = splitter.split_text(text)
    embeddings = OllamaEmbeddings(model="llama3.2:latest")
    vector_store = Chroma.from_texts(texts, embedding=embeddings, collection_name="pdf_collection")
    return vector_store


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

def main():
    st.title("üìÑ PDF-–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä")

    if not TELEGRAM_CHAT_ID:
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–∞—à Telegram Chat ID –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π.")
        st.stop()

    # –ó–∞–≥—Ä—É–∑–∫–∞ PDF-—Ñ–∞–π–ª–∞
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF-—Ñ–∞–π–ª", type=["pdf"])
    pdf_text = extract_text_from_pdf(uploaded_file) if uploaded_file else None
    
    user_input = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å:")

    # –ï—Å–ª–∏ –≤ session_state –µ—â—ë –Ω–µ—Ç –±—É—Ñ–µ—Ä–∞, —Å–æ–∑–¥–∞—ë–º –µ–≥–æ (–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏)
    if "conversation_memory" not in st.session_state:
        st.session_state.conversation_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # –†–∞–∑–º–µ—â–∞–µ–º –∫–Ω–æ–ø–∫–∏: –æ—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞, –æ—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –±—É—Ñ–µ—Ä–∞
    col1, col2, col3 = st.columns(3)
    send_query = col1.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å")
    clear_cache = col2.button("–û—á–∏—Å—Ç–∏—Ç—å –∫–µ—à LLM")
    clear_buffer = col3.button("–û—á–∏—Å—Ç–∏—Ç—å –±—É—Ñ–µ—Ä")

    if clear_cache:
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–µ—à, —Ç–µ–º —Å–∞–º—ã–º –æ—á–∏—â–∞—è –µ–≥–æ
        set_llm_cache(InMemoryCache())
        st.success("–ö–µ—à LLM –æ—á–∏—â–µ–Ω!")

    if clear_buffer:
        # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –¥–∏–∞–ª–æ–≥–∞ (–∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å)
        st.session_state.conversation_memory.clear()
        st.success("–ë—É—Ñ–µ—Ä –æ—á–∏—â–µ–Ω!")

    if send_query:
        if not user_input.strip():
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å.")
            return
        if not pdf_text:
            st.warning("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –Ω–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω.")
            return

        filtered_input = filter_swear_words_langchain(user_input)
        if filtered_input is None:
            return

        with st.spinner("–ò–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞..."):
            vector_store = create_vector_store(pdf_text)
        
        llm = OllamaLLM(model="llama3.2:latest", callbacks=[TelegramCallbackHandler()])
        
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm,
            retriever=vector_store.as_retriever(),
            memory=st.session_state.conversation_memory
        )
        
        st.write("üì¢ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM...")
        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
            response = qa_chain({"question": filtered_input})
        
        answer = response.get("answer", "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞")
        st.markdown("### ‚úÖ –û—Ç–≤–µ—Ç:")
        st.write(answer)
        
        if TELEGRAM_CHAT_ID:
            send_telegram_message_sync(TELEGRAM_CHAT_ID, f"‚úÖ –û—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å:\n\n{answer}")
            
if __name__ == "__main__":
    main()
